{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ゼロから作る Deep Learning2 第1章",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMBg5KkkDb6qfpX1jiGnuHb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/opticalcode/deeplearning2/blob/master/%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8B_Deep_Learning2_%E7%AC%AC1%E7%AB%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoHM1LyhUz9d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "fc36aba3-568a-4623-ea9f-7ecf1f77b1d5"
      },
      "source": [
        "# Google driveをマウントする\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "179dJADpVgFI",
        "colab_type": "text"
      },
      "source": [
        "1章 ニューラルネットワークの復習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GptP2bJcYkZP",
        "colab_type": "text"
      },
      "source": [
        "1.1 数学とPythonの復習\n",
        "* ベクトルや行列を拡張させて、N次元の数集まりをテンソルという"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiytAAY_Y_Ar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5503191a-e390-450c-cf68-37ce58755837"
      },
      "source": [
        "import numpy as np\n",
        "x = np.array([1,2,3])\n",
        "x.__class__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2P_WTnJZeae",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "46d6fee0-3d3b-495e-ecf9-7e5c326dd1fc"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80-QunriZiLL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "22cbea9b-e6b4-4070-d5ef-736919adbb7f"
      },
      "source": [
        "x.ndim"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-fQ15QbZmYf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "210832c5-158a-4df6-a6c6-27da618ff877"
      },
      "source": [
        "W = np.array([[1,2,3],[4,4,6]])\n",
        "W.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nvaUMgzZ64F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "73a7ac67-2c2b-4672-a59d-0cdb86973429"
      },
      "source": [
        "W.ndim"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4t-c_W9aDCD",
        "colab_type": "text"
      },
      "source": [
        "1.1.2 行列の要素ごとの演算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0Ja0FIeaLWE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "dc268047-4bfa-432c-f765-0eb4c1f702e2"
      },
      "source": [
        "W = np.array([[1,2,3],[4,5,6]])\n",
        "X = np.array([[0,1,2],[3,4,5]])\n",
        "W + X"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  3,  5],\n",
              "       [ 7,  9, 11]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOXzHWlOalfx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1f7ea606-1c6d-4f44-8248-478ad57fb4ea"
      },
      "source": [
        "W * X"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  2,  6],\n",
              "       [12, 20, 30]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxyF-4qYarbI",
        "colab_type": "text"
      },
      "source": [
        "1.1.3 ブロードキャスト"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDVMi7QAawVd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a07750ac-26d0-428b-9feb-6a0f71a89867"
      },
      "source": [
        "A = np.array([[1,2],[3,4]])\n",
        "A * 10"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10, 20],\n",
              "       [30, 40]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sfm0h4UNa8fb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "088b3c63-c875-4a25-cbb8-d70a6568efd9"
      },
      "source": [
        "A = np.array([[1,2],[3,4]])\n",
        "b = np.array([10,20])\n",
        "A * b"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10, 40],\n",
              "       [30, 80]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9QhfXrKbSjy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "40e85aa5-50bc-46f5-fbd2-1b84e496b8f5"
      },
      "source": [
        "a = np.array([1,2,3])\n",
        "b = np.array([4,5,6])\n",
        "np.dot(a,b)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGoOxXT_bk8o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "35b5324f-ad75-4a2e-f2c3-e658c9371b54"
      },
      "source": [
        "A = np.array([[1,2],[3,4]])\n",
        "B = np.array([[5,6],[7,8]])\n",
        "np.dot(A,B)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[19, 22],\n",
              "       [43, 50]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbHyyELzb4Kt",
        "colab_type": "text"
      },
      "source": [
        "1.2.5 行列の形状チェック\n",
        "* 行列の積では、対応する次元の要素数を一致させる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TYUb_KacZix",
        "colab_type": "text"
      },
      "source": [
        "1.2 ニューラルネットワークの推論\n",
        "* ニューラルネットワークの処理は「学習」と「推論」に分かれる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lI0QifmdQmI",
        "colab_type": "text"
      },
      "source": [
        "1.2.1 ニューラルネットワークの推論の全体図"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaqV505KdcoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "W1 = np.random.randn(2, 4)\n",
        "b1 = np.random.randn(4)\n",
        "x = np.random.randn(10, 2)\n",
        "h = np.dot(x, W1) + b1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9CqtzWleWQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "a = sigmoid(h)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTCbkhTee0G8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "x = np.random.randn(10, 2)\n",
        "W1 = np.random.randn(2, 4)\n",
        "b1 = np.random.randn(4)\n",
        "W2 = np.random.randn(4, 3)\n",
        "b2 = np.random.randn(3)\n",
        "\n",
        "h = np.dot(x, W1) + b1\n",
        "a = sigmoid(h)\n",
        "s = np.dot(a, W2) + b2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "248BfiApbfcU",
        "colab_type": "text"
      },
      "source": [
        "1.2.2 レイヤとしてのクラス化と順伝播の実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l90NEcX5cBP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.params = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 1 / (1 + np.exp(-x))   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBaJkXWYcrPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.params = [W, b]\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, b = self.params\n",
        "\n",
        "        out = np.dot(x, W) + b\n",
        "        return out        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysbD4Y49d3CL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        I, H, O = input_size, hidden_size, output_size\n",
        "\n",
        "        #　重みとバイアスの初期化\n",
        "        W1 = np.random.randn(I, H)\n",
        "        b1 = np.random.randn(H)\n",
        "        W2 = np.random.randn(H, O)\n",
        "        b2 = np.random.randn(O)\n",
        "\n",
        "        #　レイヤの生成\n",
        "        self.layers = [\n",
        "            Affine(W1, b1),\n",
        "            Sigmoid(),\n",
        "            Affine(W2, b2)          \n",
        "        ]\n",
        "\n",
        "        # 全ての重みをリストにまとめる\n",
        "        self.params = []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "\n",
        "    def predict(self, x):\n",
        "       for layer in self.layers:        \n",
        "           x = layer.forward(x)\n",
        "       return x    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCcU5A6DgNtn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6e59aadc-84bb-4d50-80a5-427836a1d198"
      },
      "source": [
        "a = ['A','B']\n",
        "a += ['C','D']\n",
        "a"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A', 'B', 'C', 'D']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7AZK5Pegghc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.random.randn(10, 2)\n",
        "model = TwoLayerNet(2, 4, 3)\n",
        "s = model.predict(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE8PsmgfhawQ",
        "colab_type": "text"
      },
      "source": [
        "1.3 ニューラルネットワークの学習\n",
        "* 学習されたパラメータを利用して推論を行う"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFbjopd7h9_i",
        "colab_type": "text"
      },
      "source": [
        "1.3.1 損失関数\n",
        "* 損失関数として交差エントロピー誤差を用いる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBgWghhjidig",
        "colab_type": "text"
      },
      "source": [
        "1.3.2 微分と勾配\n",
        "* ニューラルネットワークの学習の目標は、損失をできるだけ小さくすること。このとき重要になのが「微分」であり、「勾配」。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcEhgJJllrEK",
        "colab_type": "text"
      },
      "source": [
        "1.3.3 チェインルール\n",
        "* 誤差伝播法を理解する上で、キーとなるのがチェインルール（連鎖率）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2enxtV-jmJxF",
        "colab_type": "text"
      },
      "source": [
        "1.3.4 計算グラフ\n",
        "* 計算グラフは、計算を視覚的に表すもの。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldA5u2qem_4Y",
        "colab_type": "text"
      },
      "source": [
        "1.3.4.1 乗算ノード\n",
        "* 乗算ノードは、z=x * y という計算"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ1ptuQKnW_s",
        "colab_type": "text"
      },
      "source": [
        "1.3.4.2 分岐ノード\n",
        "* 分岐ノードは、分岐するノードです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSX1LZzDnsmY",
        "colab_type": "text"
      },
      "source": [
        "１.3.4.3 Repeatノード\n",
        "* 分岐ノードは2つの分岐だが、これを一般化させるとN個の分岐が考えらる。これをRepeatノードという。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yxd38EYoMI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "D, N = 8, 7\n",
        "x = np.random.randn(1, D)\n",
        "y = np.repeat(x, N, axis=0)\n",
        "\n",
        "dy = np.random.randn(N, D)\n",
        "dx = np.sum(dy, axis=0, keepdims=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xfLXj9Ypdi4",
        "colab_type": "text"
      },
      "source": [
        "1.3.4.4 Sumノード\n",
        "* Sumノードは汎用的な加算ノードです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykInGVwDpulC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "D, N = 8, 7\n",
        "x = np.random.randn(N, D)\n",
        "y = np.sum(x, axis=0, keepdims=True)\n",
        "\n",
        "dy = np.random.randn(1, D)\n",
        "dx = np.repeat(dy, N, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMXeHOfuqW7D",
        "colab_type": "text"
      },
      "source": [
        "1.3.4.5 MatMulノード\n",
        "* 本書では、行列の積をMatMulノードとします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A4rMhY1quYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Matmul:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, = self.params\n",
        "        out = np.dot(x, W)\n",
        "        self.x = x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        W, = self.params\n",
        "        dx = np.dot(dout, W.t)\n",
        "        dW = np.dot(self.x.T, dout)\n",
        "        self.grads[0][...]= dW\n",
        "        return dx        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbjaHX5-sF1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = np.array([1, 2, 3])\n",
        "b = np.array([4, 5, 6])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaCHIfYdsYP0",
        "colab_type": "text"
      },
      "source": [
        "1.3.5 勾配の導出と逆伝播の実装"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7P7giHTshEN",
        "colab_type": "text"
      },
      "source": [
        "1.3.5.1 Sigmoidレイヤ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xpnel5zspJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.params, set.grads =[], []\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = 1 / (1 + np.exp(-x))\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out)\n",
        "        return dx        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmHmydkOtfJs",
        "colab_type": "text"
      },
      "source": [
        "1.3.5.2 Affineレイヤ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX_clz-otlqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.params = [W, b]\n",
        "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, b = self.params\n",
        "        out =np.dot(x, W) + b\n",
        "        self.x = x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        W, b = self.params\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dW = np.dot(self.x.T, dout)\n",
        "        db = np.sum(dout, axis=0) \n",
        "\n",
        "        self.grads[0][...] = dW\n",
        "        self.grads[1][...] = db\n",
        "        return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21zsWSyKvTBl",
        "colab_type": "text"
      },
      "source": [
        "1.3.5.3 Softmax with Lossレイヤ\n",
        "* softmax関数と交差エントロピー誤差を合わせてそftmax with Lossレイヤとして実装します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emNKrRJIwNUg",
        "colab_type": "text"
      },
      "source": [
        "1.3.6 重みの更新\n",
        "* 誤差逆伝播法によって勾配を求めることができたら、その勾配を使ってニューラルネットワークの学習を行います。\n",
        "* パラメーターの更新に勾配降下法を使用します。\n",
        "* ここではSGD（確率的勾配降下法）を使用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FDqkLDExHqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SGD:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = %ldir\n",
        "    def update(self, params, grads):\n",
        "        for i in range(len(params)):\n",
        "            params[i] -= self.lr * grads[i]    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDrv5HZsyIoy",
        "colab_type": "text"
      },
      "source": [
        "1.4 ニューラルネットワークで問題を解く\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOOFLpIGV_Ue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/deep-learning-from-scratch-2-master')\n",
        "from dataset import spiral"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3OcdngwXDFN",
        "colab_type": "code",
        "outputId": "37e3e28d-6a7a-4c8c-eab4-fca1816a0fff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x, t = spiral.load_data()\n",
        "print('x', x.shape)\n",
        "print('t', t.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x (300, 2)\n",
            "t (300, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAdU4s3WZjZi",
        "colab_type": "text"
      },
      "source": [
        "1.4.2 ニューラルネットワークの実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "390fpxXRapOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/deep-learning-from-scratch-2-master/ch01')\n",
        "import numpy as np\n",
        "from common.layers import Affine, Sigmoid, SoftmaxWithLoss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XSF6yW9b7Iu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        I, H, O = input_size, hidden_size, output_size\n",
        "\n",
        "        #重みとバイアス初期化\n",
        "        W1 = 0.01 * np.random.randn(I, H)\n",
        "        b1 = np.zeros(H)\n",
        "        W2 = 0.01 * np.random.randn(H, O)\n",
        "        b2 = np.zeros(O)\n",
        "\n",
        "        #レイヤの生成\n",
        "        self.layers = [\n",
        "            Affine(W1, b1),\n",
        "            Sigmoid(),\n",
        "            Affine(W2, b2)           \n",
        "        ]\n",
        "        self.loss_layer = SoftmaxWithLoss()\n",
        "\n",
        "        #すべての重みと勾配をリストにまとめる\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        score = self.predict(x)\n",
        "        loss = self.predict(x)\n",
        "        loss = self.loss_layer.forward(score, t)\n",
        "        return x\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NZ99W8UgtpW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bcaf5881-c14a-4cac-d2e7-60b795d0a908"
      },
      "source": [
        "#学習用のソースコード\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/deep-learning-from-scratch-2-master/ch01')\n",
        "import numpy as np\n",
        "from common.optimizer import SGD\n",
        "from dataset import spiral\n",
        "import matplotlib.pyplot as plt\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "#❶ハイパーパラメータの設定\n",
        "max_epoch = 300\n",
        "batch_size = 30\n",
        "hidden_size = 10\n",
        "learning_rate =1.0\n",
        "\n",
        "#❷データの読み込み、モデルとオプティマイザの生成\n",
        "x, t = spiral.load_data()\n",
        "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
        "optimizer = SGD(lr=learning_rate)\n",
        "\n",
        "# 学習で使用する変数\n",
        "data_size = len(x)\n",
        "max_iters = data_size // batch_size\n",
        "total_loss = 0\n",
        "loss_count = 0\n",
        "loss_list = []\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    #❸データのシャッフル\n",
        "    idx = np.random.permutation(data_size)\n",
        "    x = x[idx]\n",
        "    t = t[idx]\n",
        "\n",
        "    for iters in range(max_iters):\n",
        "        batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
        "        batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
        "\n",
        "    #❹勾配を求め、パラメータを更新\n",
        "    loss = model.forward(batch_x, batch_t)\n",
        "    model.backward()\n",
        "    optimizer.update(model.params, model.grads)\n",
        "\n",
        "    total_loss += loss\n",
        "    loss_count += 1\n",
        "\n",
        "    #❺定期的に学習経過を出力\n",
        "    if (iters+1) % 10 == 0:\n",
        "        avg_loss = total_loss / loss_count\n",
        "        print('\\ epoch %d \\ iter %d / %d \\ loss %.2f'\n",
        "              % (epoch + 1, iters + 1, max_iters, avg_loss))\n",
        "        loss_list.append(avg_loss)\n",
        "        total_loss, loss_count = 0, 0"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\\ epoch 1 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 2 \\ iter 10 / 10 \\ loss 1.12\n",
            "\\ epoch 3 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 4 \\ iter 10 / 10 \\ loss 1.08\n",
            "\\ epoch 5 \\ iter 10 / 10 \\ loss 1.18\n",
            "\\ epoch 6 \\ iter 10 / 10 \\ loss 1.22\n",
            "\\ epoch 7 \\ iter 10 / 10 \\ loss 1.24\n",
            "\\ epoch 8 \\ iter 10 / 10 \\ loss 1.13\n",
            "\\ epoch 9 \\ iter 10 / 10 \\ loss 1.12\n",
            "\\ epoch 10 \\ iter 10 / 10 \\ loss 1.20\n",
            "\\ epoch 11 \\ iter 10 / 10 \\ loss 1.22\n",
            "\\ epoch 12 \\ iter 10 / 10 \\ loss 1.22\n",
            "\\ epoch 13 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 14 \\ iter 10 / 10 \\ loss 1.12\n",
            "\\ epoch 15 \\ iter 10 / 10 \\ loss 1.24\n",
            "\\ epoch 16 \\ iter 10 / 10 \\ loss 1.32\n",
            "\\ epoch 17 \\ iter 10 / 10 \\ loss 1.16\n",
            "\\ epoch 18 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 19 \\ iter 10 / 10 \\ loss 1.13\n",
            "\\ epoch 20 \\ iter 10 / 10 \\ loss 1.12\n",
            "\\ epoch 21 \\ iter 10 / 10 \\ loss 1.06\n",
            "\\ epoch 22 \\ iter 10 / 10 \\ loss 1.42\n",
            "\\ epoch 23 \\ iter 10 / 10 \\ loss 1.11\n",
            "\\ epoch 24 \\ iter 10 / 10 \\ loss 1.17\n",
            "\\ epoch 25 \\ iter 10 / 10 \\ loss 1.06\n",
            "\\ epoch 26 \\ iter 10 / 10 \\ loss 1.24\n",
            "\\ epoch 27 \\ iter 10 / 10 \\ loss 1.11\n",
            "\\ epoch 28 \\ iter 10 / 10 \\ loss 1.11\n",
            "\\ epoch 29 \\ iter 10 / 10 \\ loss 1.11\n",
            "\\ epoch 30 \\ iter 10 / 10 \\ loss 1.13\n",
            "\\ epoch 31 \\ iter 10 / 10 \\ loss 1.13\n",
            "\\ epoch 32 \\ iter 10 / 10 \\ loss 1.24\n",
            "\\ epoch 33 \\ iter 10 / 10 \\ loss 1.20\n",
            "\\ epoch 34 \\ iter 10 / 10 \\ loss 1.08\n",
            "\\ epoch 35 \\ iter 10 / 10 \\ loss 1.15\n",
            "\\ epoch 36 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 37 \\ iter 10 / 10 \\ loss 1.14\n",
            "\\ epoch 38 \\ iter 10 / 10 \\ loss 1.13\n",
            "\\ epoch 39 \\ iter 10 / 10 \\ loss 1.15\n",
            "\\ epoch 40 \\ iter 10 / 10 \\ loss 1.34\n",
            "\\ epoch 41 \\ iter 10 / 10 \\ loss 1.16\n",
            "\\ epoch 42 \\ iter 10 / 10 \\ loss 1.13\n",
            "\\ epoch 43 \\ iter 10 / 10 \\ loss 1.20\n",
            "\\ epoch 44 \\ iter 10 / 10 \\ loss 1.12\n",
            "\\ epoch 45 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 46 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 47 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 48 \\ iter 10 / 10 \\ loss 1.20\n",
            "\\ epoch 49 \\ iter 10 / 10 \\ loss 1.29\n",
            "\\ epoch 50 \\ iter 10 / 10 \\ loss 1.18\n",
            "\\ epoch 51 \\ iter 10 / 10 \\ loss 1.12\n",
            "\\ epoch 52 \\ iter 10 / 10 \\ loss 1.14\n",
            "\\ epoch 53 \\ iter 10 / 10 \\ loss 1.12\n",
            "\\ epoch 54 \\ iter 10 / 10 \\ loss 1.17\n",
            "\\ epoch 55 \\ iter 10 / 10 \\ loss 1.23\n",
            "\\ epoch 56 \\ iter 10 / 10 \\ loss 1.12\n",
            "\\ epoch 57 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 58 \\ iter 10 / 10 \\ loss 1.17\n",
            "\\ epoch 59 \\ iter 10 / 10 \\ loss 1.26\n",
            "\\ epoch 60 \\ iter 10 / 10 \\ loss 1.09\n",
            "\\ epoch 61 \\ iter 10 / 10 \\ loss 1.14\n",
            "\\ epoch 62 \\ iter 10 / 10 \\ loss 1.16\n",
            "\\ epoch 63 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 64 \\ iter 10 / 10 \\ loss 1.12\n",
            "\\ epoch 65 \\ iter 10 / 10 \\ loss 1.08\n",
            "\\ epoch 66 \\ iter 10 / 10 \\ loss 1.15\n",
            "\\ epoch 67 \\ iter 10 / 10 \\ loss 1.07\n",
            "\\ epoch 68 \\ iter 10 / 10 \\ loss 1.17\n",
            "\\ epoch 69 \\ iter 10 / 10 \\ loss 1.09\n",
            "\\ epoch 70 \\ iter 10 / 10 \\ loss 1.18\n",
            "\\ epoch 71 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 72 \\ iter 10 / 10 \\ loss 1.12\n",
            "\\ epoch 73 \\ iter 10 / 10 \\ loss 1.11\n",
            "\\ epoch 74 \\ iter 10 / 10 \\ loss 1.17\n",
            "\\ epoch 75 \\ iter 10 / 10 \\ loss 1.17\n",
            "\\ epoch 76 \\ iter 10 / 10 \\ loss 1.22\n",
            "\\ epoch 77 \\ iter 10 / 10 \\ loss 1.22\n",
            "\\ epoch 78 \\ iter 10 / 10 \\ loss 1.06\n",
            "\\ epoch 79 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 80 \\ iter 10 / 10 \\ loss 1.12\n",
            "\\ epoch 81 \\ iter 10 / 10 \\ loss 1.21\n",
            "\\ epoch 82 \\ iter 10 / 10 \\ loss 1.15\n",
            "\\ epoch 83 \\ iter 10 / 10 \\ loss 1.07\n",
            "\\ epoch 84 \\ iter 10 / 10 \\ loss 1.15\n",
            "\\ epoch 85 \\ iter 10 / 10 \\ loss 1.08\n",
            "\\ epoch 86 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 87 \\ iter 10 / 10 \\ loss 1.12\n",
            "\\ epoch 88 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 89 \\ iter 10 / 10 \\ loss 1.08\n",
            "\\ epoch 90 \\ iter 10 / 10 \\ loss 1.14\n",
            "\\ epoch 91 \\ iter 10 / 10 \\ loss 1.08\n",
            "\\ epoch 92 \\ iter 10 / 10 \\ loss 1.11\n",
            "\\ epoch 93 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 94 \\ iter 10 / 10 \\ loss 1.11\n",
            "\\ epoch 95 \\ iter 10 / 10 \\ loss 1.15\n",
            "\\ epoch 96 \\ iter 10 / 10 \\ loss 1.13\n",
            "\\ epoch 97 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 98 \\ iter 10 / 10 \\ loss 1.07\n",
            "\\ epoch 99 \\ iter 10 / 10 \\ loss 1.17\n",
            "\\ epoch 100 \\ iter 10 / 10 \\ loss 1.21\n",
            "\\ epoch 101 \\ iter 10 / 10 \\ loss 1.15\n",
            "\\ epoch 102 \\ iter 10 / 10 \\ loss 1.11\n",
            "\\ epoch 103 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 104 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 105 \\ iter 10 / 10 \\ loss 1.18\n",
            "\\ epoch 106 \\ iter 10 / 10 \\ loss 1.09\n",
            "\\ epoch 107 \\ iter 10 / 10 \\ loss 1.11\n",
            "\\ epoch 108 \\ iter 10 / 10 \\ loss 1.15\n",
            "\\ epoch 109 \\ iter 10 / 10 \\ loss 1.28\n",
            "\\ epoch 110 \\ iter 10 / 10 \\ loss 1.14\n",
            "\\ epoch 111 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 112 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 113 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 114 \\ iter 10 / 10 \\ loss 1.08\n",
            "\\ epoch 115 \\ iter 10 / 10 \\ loss 1.08\n",
            "\\ epoch 116 \\ iter 10 / 10 \\ loss 1.13\n",
            "\\ epoch 117 \\ iter 10 / 10 \\ loss 1.12\n",
            "\\ epoch 118 \\ iter 10 / 10 \\ loss 1.08\n",
            "\\ epoch 119 \\ iter 10 / 10 \\ loss 1.09\n",
            "\\ epoch 120 \\ iter 10 / 10 \\ loss 1.11\n",
            "\\ epoch 121 \\ iter 10 / 10 \\ loss 1.06\n",
            "\\ epoch 122 \\ iter 10 / 10 \\ loss 1.13\n",
            "\\ epoch 123 \\ iter 10 / 10 \\ loss 1.12\n",
            "\\ epoch 124 \\ iter 10 / 10 \\ loss 1.05\n",
            "\\ epoch 125 \\ iter 10 / 10 \\ loss 1.05\n",
            "\\ epoch 126 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 127 \\ iter 10 / 10 \\ loss 1.06\n",
            "\\ epoch 128 \\ iter 10 / 10 \\ loss 1.06\n",
            "\\ epoch 129 \\ iter 10 / 10 \\ loss 1.06\n",
            "\\ epoch 130 \\ iter 10 / 10 \\ loss 1.07\n",
            "\\ epoch 131 \\ iter 10 / 10 \\ loss 1.05\n",
            "\\ epoch 132 \\ iter 10 / 10 \\ loss 1.04\n",
            "\\ epoch 133 \\ iter 10 / 10 \\ loss 1.10\n",
            "\\ epoch 134 \\ iter 10 / 10 \\ loss 1.04\n",
            "\\ epoch 135 \\ iter 10 / 10 \\ loss 1.02\n",
            "\\ epoch 136 \\ iter 10 / 10 \\ loss 1.09\n",
            "\\ epoch 137 \\ iter 10 / 10 \\ loss 1.08\n",
            "\\ epoch 138 \\ iter 10 / 10 \\ loss 0.98\n",
            "\\ epoch 139 \\ iter 10 / 10 \\ loss 1.18\n",
            "\\ epoch 140 \\ iter 10 / 10 \\ loss 0.98\n",
            "\\ epoch 141 \\ iter 10 / 10 \\ loss 1.03\n",
            "\\ epoch 142 \\ iter 10 / 10 \\ loss 1.05\n",
            "\\ epoch 143 \\ iter 10 / 10 \\ loss 1.04\n",
            "\\ epoch 144 \\ iter 10 / 10 \\ loss 1.06\n",
            "\\ epoch 145 \\ iter 10 / 10 \\ loss 1.01\n",
            "\\ epoch 146 \\ iter 10 / 10 \\ loss 1.07\n",
            "\\ epoch 147 \\ iter 10 / 10 \\ loss 1.04\n",
            "\\ epoch 148 \\ iter 10 / 10 \\ loss 1.01\n",
            "\\ epoch 149 \\ iter 10 / 10 \\ loss 1.00\n",
            "\\ epoch 150 \\ iter 10 / 10 \\ loss 1.02\n",
            "\\ epoch 151 \\ iter 10 / 10 \\ loss 1.04\n",
            "\\ epoch 152 \\ iter 10 / 10 \\ loss 0.96\n",
            "\\ epoch 153 \\ iter 10 / 10 \\ loss 1.01\n",
            "\\ epoch 154 \\ iter 10 / 10 \\ loss 1.00\n",
            "\\ epoch 155 \\ iter 10 / 10 \\ loss 1.01\n",
            "\\ epoch 156 \\ iter 10 / 10 \\ loss 0.98\n",
            "\\ epoch 157 \\ iter 10 / 10 \\ loss 1.11\n",
            "\\ epoch 158 \\ iter 10 / 10 \\ loss 1.04\n",
            "\\ epoch 159 \\ iter 10 / 10 \\ loss 0.95\n",
            "\\ epoch 160 \\ iter 10 / 10 \\ loss 0.93\n",
            "\\ epoch 161 \\ iter 10 / 10 \\ loss 1.03\n",
            "\\ epoch 162 \\ iter 10 / 10 \\ loss 0.97\n",
            "\\ epoch 163 \\ iter 10 / 10 \\ loss 1.07\n",
            "\\ epoch 164 \\ iter 10 / 10 \\ loss 0.96\n",
            "\\ epoch 165 \\ iter 10 / 10 \\ loss 0.91\n",
            "\\ epoch 166 \\ iter 10 / 10 \\ loss 0.95\n",
            "\\ epoch 167 \\ iter 10 / 10 \\ loss 0.96\n",
            "\\ epoch 168 \\ iter 10 / 10 \\ loss 0.91\n",
            "\\ epoch 169 \\ iter 10 / 10 \\ loss 0.95\n",
            "\\ epoch 170 \\ iter 10 / 10 \\ loss 0.99\n",
            "\\ epoch 171 \\ iter 10 / 10 \\ loss 0.87\n",
            "\\ epoch 172 \\ iter 10 / 10 \\ loss 1.01\n",
            "\\ epoch 173 \\ iter 10 / 10 \\ loss 0.83\n",
            "\\ epoch 174 \\ iter 10 / 10 \\ loss 1.05\n",
            "\\ epoch 175 \\ iter 10 / 10 \\ loss 0.89\n",
            "\\ epoch 176 \\ iter 10 / 10 \\ loss 0.90\n",
            "\\ epoch 177 \\ iter 10 / 10 \\ loss 0.94\n",
            "\\ epoch 178 \\ iter 10 / 10 \\ loss 0.90\n",
            "\\ epoch 179 \\ iter 10 / 10 \\ loss 0.92\n",
            "\\ epoch 180 \\ iter 10 / 10 \\ loss 0.85\n",
            "\\ epoch 181 \\ iter 10 / 10 \\ loss 0.83\n",
            "\\ epoch 182 \\ iter 10 / 10 \\ loss 0.90\n",
            "\\ epoch 183 \\ iter 10 / 10 \\ loss 0.89\n",
            "\\ epoch 184 \\ iter 10 / 10 \\ loss 0.88\n",
            "\\ epoch 185 \\ iter 10 / 10 \\ loss 0.87\n",
            "\\ epoch 186 \\ iter 10 / 10 \\ loss 0.86\n",
            "\\ epoch 187 \\ iter 10 / 10 \\ loss 0.82\n",
            "\\ epoch 188 \\ iter 10 / 10 \\ loss 0.81\n",
            "\\ epoch 189 \\ iter 10 / 10 \\ loss 0.84\n",
            "\\ epoch 190 \\ iter 10 / 10 \\ loss 0.98\n",
            "\\ epoch 191 \\ iter 10 / 10 \\ loss 1.04\n",
            "\\ epoch 192 \\ iter 10 / 10 \\ loss 0.93\n",
            "\\ epoch 193 \\ iter 10 / 10 \\ loss 0.84\n",
            "\\ epoch 194 \\ iter 10 / 10 \\ loss 0.88\n",
            "\\ epoch 195 \\ iter 10 / 10 \\ loss 0.73\n",
            "\\ epoch 196 \\ iter 10 / 10 \\ loss 0.89\n",
            "\\ epoch 197 \\ iter 10 / 10 \\ loss 0.89\n",
            "\\ epoch 198 \\ iter 10 / 10 \\ loss 0.87\n",
            "\\ epoch 199 \\ iter 10 / 10 \\ loss 0.74\n",
            "\\ epoch 200 \\ iter 10 / 10 \\ loss 0.90\n",
            "\\ epoch 201 \\ iter 10 / 10 \\ loss 0.82\n",
            "\\ epoch 202 \\ iter 10 / 10 \\ loss 0.91\n",
            "\\ epoch 203 \\ iter 10 / 10 \\ loss 0.94\n",
            "\\ epoch 204 \\ iter 10 / 10 \\ loss 0.84\n",
            "\\ epoch 205 \\ iter 10 / 10 \\ loss 0.84\n",
            "\\ epoch 206 \\ iter 10 / 10 \\ loss 1.02\n",
            "\\ epoch 207 \\ iter 10 / 10 \\ loss 0.88\n",
            "\\ epoch 208 \\ iter 10 / 10 \\ loss 0.87\n",
            "\\ epoch 209 \\ iter 10 / 10 \\ loss 0.70\n",
            "\\ epoch 210 \\ iter 10 / 10 \\ loss 0.90\n",
            "\\ epoch 211 \\ iter 10 / 10 \\ loss 0.77\n",
            "\\ epoch 212 \\ iter 10 / 10 \\ loss 0.77\n",
            "\\ epoch 213 \\ iter 10 / 10 \\ loss 0.74\n",
            "\\ epoch 214 \\ iter 10 / 10 \\ loss 0.86\n",
            "\\ epoch 215 \\ iter 10 / 10 \\ loss 0.79\n",
            "\\ epoch 216 \\ iter 10 / 10 \\ loss 0.90\n",
            "\\ epoch 217 \\ iter 10 / 10 \\ loss 0.74\n",
            "\\ epoch 218 \\ iter 10 / 10 \\ loss 0.86\n",
            "\\ epoch 219 \\ iter 10 / 10 \\ loss 0.67\n",
            "\\ epoch 220 \\ iter 10 / 10 \\ loss 0.84\n",
            "\\ epoch 221 \\ iter 10 / 10 \\ loss 0.69\n",
            "\\ epoch 222 \\ iter 10 / 10 \\ loss 0.70\n",
            "\\ epoch 223 \\ iter 10 / 10 \\ loss 0.77\n",
            "\\ epoch 224 \\ iter 10 / 10 \\ loss 0.88\n",
            "\\ epoch 225 \\ iter 10 / 10 \\ loss 0.79\n",
            "\\ epoch 226 \\ iter 10 / 10 \\ loss 0.68\n",
            "\\ epoch 227 \\ iter 10 / 10 \\ loss 0.72\n",
            "\\ epoch 228 \\ iter 10 / 10 \\ loss 0.65\n",
            "\\ epoch 229 \\ iter 10 / 10 \\ loss 0.70\n",
            "\\ epoch 230 \\ iter 10 / 10 \\ loss 0.77\n",
            "\\ epoch 231 \\ iter 10 / 10 \\ loss 0.79\n",
            "\\ epoch 232 \\ iter 10 / 10 \\ loss 0.70\n",
            "\\ epoch 233 \\ iter 10 / 10 \\ loss 0.80\n",
            "\\ epoch 234 \\ iter 10 / 10 \\ loss 0.86\n",
            "\\ epoch 235 \\ iter 10 / 10 \\ loss 0.73\n",
            "\\ epoch 236 \\ iter 10 / 10 \\ loss 0.72\n",
            "\\ epoch 237 \\ iter 10 / 10 \\ loss 0.75\n",
            "\\ epoch 238 \\ iter 10 / 10 \\ loss 0.83\n",
            "\\ epoch 239 \\ iter 10 / 10 \\ loss 0.64\n",
            "\\ epoch 240 \\ iter 10 / 10 \\ loss 0.80\n",
            "\\ epoch 241 \\ iter 10 / 10 \\ loss 0.71\n",
            "\\ epoch 242 \\ iter 10 / 10 \\ loss 0.59\n",
            "\\ epoch 243 \\ iter 10 / 10 \\ loss 0.69\n",
            "\\ epoch 244 \\ iter 10 / 10 \\ loss 0.79\n",
            "\\ epoch 245 \\ iter 10 / 10 \\ loss 0.68\n",
            "\\ epoch 246 \\ iter 10 / 10 \\ loss 0.80\n",
            "\\ epoch 247 \\ iter 10 / 10 \\ loss 0.82\n",
            "\\ epoch 248 \\ iter 10 / 10 \\ loss 0.85\n",
            "\\ epoch 249 \\ iter 10 / 10 \\ loss 0.63\n",
            "\\ epoch 250 \\ iter 10 / 10 \\ loss 0.84\n",
            "\\ epoch 251 \\ iter 10 / 10 \\ loss 0.79\n",
            "\\ epoch 252 \\ iter 10 / 10 \\ loss 0.77\n",
            "\\ epoch 253 \\ iter 10 / 10 \\ loss 0.78\n",
            "\\ epoch 254 \\ iter 10 / 10 \\ loss 0.63\n",
            "\\ epoch 255 \\ iter 10 / 10 \\ loss 0.74\n",
            "\\ epoch 256 \\ iter 10 / 10 \\ loss 0.74\n",
            "\\ epoch 257 \\ iter 10 / 10 \\ loss 0.70\n",
            "\\ epoch 258 \\ iter 10 / 10 \\ loss 0.86\n",
            "\\ epoch 259 \\ iter 10 / 10 \\ loss 0.79\n",
            "\\ epoch 260 \\ iter 10 / 10 \\ loss 0.93\n",
            "\\ epoch 261 \\ iter 10 / 10 \\ loss 0.85\n",
            "\\ epoch 262 \\ iter 10 / 10 \\ loss 0.71\n",
            "\\ epoch 263 \\ iter 10 / 10 \\ loss 0.77\n",
            "\\ epoch 264 \\ iter 10 / 10 \\ loss 0.76\n",
            "\\ epoch 265 \\ iter 10 / 10 \\ loss 0.71\n",
            "\\ epoch 266 \\ iter 10 / 10 \\ loss 0.83\n",
            "\\ epoch 267 \\ iter 10 / 10 \\ loss 0.80\n",
            "\\ epoch 268 \\ iter 10 / 10 \\ loss 0.88\n",
            "\\ epoch 269 \\ iter 10 / 10 \\ loss 0.63\n",
            "\\ epoch 270 \\ iter 10 / 10 \\ loss 0.80\n",
            "\\ epoch 271 \\ iter 10 / 10 \\ loss 0.72\n",
            "\\ epoch 272 \\ iter 10 / 10 \\ loss 0.76\n",
            "\\ epoch 273 \\ iter 10 / 10 \\ loss 1.12\n",
            "\\ epoch 274 \\ iter 10 / 10 \\ loss 0.83\n",
            "\\ epoch 275 \\ iter 10 / 10 \\ loss 0.66\n",
            "\\ epoch 276 \\ iter 10 / 10 \\ loss 0.66\n",
            "\\ epoch 277 \\ iter 10 / 10 \\ loss 0.69\n",
            "\\ epoch 278 \\ iter 10 / 10 \\ loss 0.67\n",
            "\\ epoch 279 \\ iter 10 / 10 \\ loss 0.75\n",
            "\\ epoch 280 \\ iter 10 / 10 \\ loss 0.72\n",
            "\\ epoch 281 \\ iter 10 / 10 \\ loss 0.75\n",
            "\\ epoch 282 \\ iter 10 / 10 \\ loss 0.83\n",
            "\\ epoch 283 \\ iter 10 / 10 \\ loss 0.75\n",
            "\\ epoch 284 \\ iter 10 / 10 \\ loss 0.82\n",
            "\\ epoch 285 \\ iter 10 / 10 \\ loss 0.71\n",
            "\\ epoch 286 \\ iter 10 / 10 \\ loss 0.77\n",
            "\\ epoch 287 \\ iter 10 / 10 \\ loss 0.75\n",
            "\\ epoch 288 \\ iter 10 / 10 \\ loss 0.71\n",
            "\\ epoch 289 \\ iter 10 / 10 \\ loss 0.83\n",
            "\\ epoch 290 \\ iter 10 / 10 \\ loss 0.58\n",
            "\\ epoch 291 \\ iter 10 / 10 \\ loss 0.62\n",
            "\\ epoch 292 \\ iter 10 / 10 \\ loss 0.84\n",
            "\\ epoch 293 \\ iter 10 / 10 \\ loss 0.90\n",
            "\\ epoch 294 \\ iter 10 / 10 \\ loss 0.76\n",
            "\\ epoch 295 \\ iter 10 / 10 \\ loss 0.76\n",
            "\\ epoch 296 \\ iter 10 / 10 \\ loss 0.91\n",
            "\\ epoch 297 \\ iter 10 / 10 \\ loss 0.74\n",
            "\\ epoch 298 \\ iter 10 / 10 \\ loss 0.79\n",
            "\\ epoch 299 \\ iter 10 / 10 \\ loss 0.90\n",
            "\\ epoch 300 \\ iter 10 / 10 \\ loss 0.69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8INTSBo8shpH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4e0a87b6-985f-4cb8-d851-bb081eeda7ae"
      },
      "source": [
        "import numpy as np\n",
        "np.random.permutation(10)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 1, 8, 4, 9, 7, 0, 2, 6, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bccNXI_NstuZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2a4287aa-2020-4e48-d3a9-20c2be4a7fe7"
      },
      "source": [
        "np.random.permutation(10)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 4, 2, 7, 8, 5, 6, 0, 9, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc3bdoG0thWw",
        "colab_type": "text"
      },
      "source": [
        "1.4.4 Trainerクラス"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6dc5rmrtnEo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f8b2b24-a9a6-4f45-ad2d-603c6ec06d61"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/deep-learning-from-scratch-2-master/ch01')\n",
        "from common.optimizer import SGD\n",
        "from common.trainer import Trainer\n",
        "from dataset import spiral\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "batch_size = 30\n",
        "hidden_size = 10\n",
        "learning_rate = 1.0\n",
        "\n",
        "x, t = spiral.load_data()\n",
        "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
        "optimizer = SGD(lr=learning_rate)\n",
        "\n",
        "trainer = Trainer(model, optimizer)\n",
        "trainer.fit(x, t, max_epoch, batch_size, eval_interval=10)\n",
        "trainer.plot()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch 1 |  iter 1 / 10 | time 0[s] | loss 1.10\n",
            "| epoch 2 |  iter 1 / 10 | time 0[s] | loss 1.12\n",
            "| epoch 3 |  iter 1 / 10 | time 0[s] | loss 1.13\n",
            "| epoch 4 |  iter 1 / 10 | time 0[s] | loss 1.12\n",
            "| epoch 5 |  iter 1 / 10 | time 0[s] | loss 1.12\n",
            "| epoch 6 |  iter 1 / 10 | time 0[s] | loss 1.10\n",
            "| epoch 7 |  iter 1 / 10 | time 0[s] | loss 1.14\n",
            "| epoch 8 |  iter 1 / 10 | time 0[s] | loss 1.16\n",
            "| epoch 9 |  iter 1 / 10 | time 0[s] | loss 1.11\n",
            "| epoch 10 |  iter 1 / 10 | time 0[s] | loss 1.12\n",
            "| epoch 11 |  iter 1 / 10 | time 0[s] | loss 1.12\n",
            "| epoch 12 |  iter 1 / 10 | time 0[s] | loss 1.12\n",
            "| epoch 13 |  iter 1 / 10 | time 0[s] | loss 1.10\n",
            "| epoch 14 |  iter 1 / 10 | time 0[s] | loss 1.09\n",
            "| epoch 15 |  iter 1 / 10 | time 0[s] | loss 1.08\n",
            "| epoch 16 |  iter 1 / 10 | time 0[s] | loss 1.04\n",
            "| epoch 17 |  iter 1 / 10 | time 0[s] | loss 1.03\n",
            "| epoch 18 |  iter 1 / 10 | time 0[s] | loss 0.94\n",
            "| epoch 19 |  iter 1 / 10 | time 0[s] | loss 0.92\n",
            "| epoch 20 |  iter 1 / 10 | time 0[s] | loss 0.92\n",
            "| epoch 21 |  iter 1 / 10 | time 0[s] | loss 0.87\n",
            "| epoch 22 |  iter 1 / 10 | time 0[s] | loss 0.85\n",
            "| epoch 23 |  iter 1 / 10 | time 0[s] | loss 0.80\n",
            "| epoch 24 |  iter 1 / 10 | time 0[s] | loss 0.79\n",
            "| epoch 25 |  iter 1 / 10 | time 0[s] | loss 0.78\n",
            "| epoch 26 |  iter 1 / 10 | time 0[s] | loss 0.83\n",
            "| epoch 27 |  iter 1 / 10 | time 0[s] | loss 0.77\n",
            "| epoch 28 |  iter 1 / 10 | time 0[s] | loss 0.76\n",
            "| epoch 29 |  iter 1 / 10 | time 0[s] | loss 0.77\n",
            "| epoch 30 |  iter 1 / 10 | time 0[s] | loss 0.76\n",
            "| epoch 31 |  iter 1 / 10 | time 0[s] | loss 0.77\n",
            "| epoch 32 |  iter 1 / 10 | time 0[s] | loss 0.75\n",
            "| epoch 33 |  iter 1 / 10 | time 0[s] | loss 0.78\n",
            "| epoch 34 |  iter 1 / 10 | time 0[s] | loss 0.77\n",
            "| epoch 35 |  iter 1 / 10 | time 0[s] | loss 0.78\n",
            "| epoch 36 |  iter 1 / 10 | time 0[s] | loss 0.74\n",
            "| epoch 37 |  iter 1 / 10 | time 0[s] | loss 0.75\n",
            "| epoch 38 |  iter 1 / 10 | time 0[s] | loss 0.77\n",
            "| epoch 39 |  iter 1 / 10 | time 0[s] | loss 0.75\n",
            "| epoch 40 |  iter 1 / 10 | time 0[s] | loss 0.73\n",
            "| epoch 41 |  iter 1 / 10 | time 0[s] | loss 0.75\n",
            "| epoch 42 |  iter 1 / 10 | time 0[s] | loss 0.76\n",
            "| epoch 43 |  iter 1 / 10 | time 0[s] | loss 0.79\n",
            "| epoch 44 |  iter 1 / 10 | time 0[s] | loss 0.74\n",
            "| epoch 45 |  iter 1 / 10 | time 0[s] | loss 0.75\n",
            "| epoch 46 |  iter 1 / 10 | time 0[s] | loss 0.73\n",
            "| epoch 47 |  iter 1 / 10 | time 0[s] | loss 0.73\n",
            "| epoch 48 |  iter 1 / 10 | time 0[s] | loss 0.73\n",
            "| epoch 49 |  iter 1 / 10 | time 0[s] | loss 0.73\n",
            "| epoch 50 |  iter 1 / 10 | time 0[s] | loss 0.72\n",
            "| epoch 51 |  iter 1 / 10 | time 0[s] | loss 0.72\n",
            "| epoch 52 |  iter 1 / 10 | time 0[s] | loss 0.72\n",
            "| epoch 53 |  iter 1 / 10 | time 0[s] | loss 0.72\n",
            "| epoch 54 |  iter 1 / 10 | time 0[s] | loss 0.74\n",
            "| epoch 55 |  iter 1 / 10 | time 0[s] | loss 0.74\n",
            "| epoch 56 |  iter 1 / 10 | time 0[s] | loss 0.73\n",
            "| epoch 57 |  iter 1 / 10 | time 0[s] | loss 0.72\n",
            "| epoch 58 |  iter 1 / 10 | time 0[s] | loss 0.69\n",
            "| epoch 59 |  iter 1 / 10 | time 0[s] | loss 0.72\n",
            "| epoch 60 |  iter 1 / 10 | time 0[s] | loss 0.70\n",
            "| epoch 61 |  iter 1 / 10 | time 0[s] | loss 0.69\n",
            "| epoch 62 |  iter 1 / 10 | time 0[s] | loss 0.71\n",
            "| epoch 63 |  iter 1 / 10 | time 0[s] | loss 0.70\n",
            "| epoch 64 |  iter 1 / 10 | time 0[s] | loss 0.71\n",
            "| epoch 65 |  iter 1 / 10 | time 0[s] | loss 0.72\n",
            "| epoch 66 |  iter 1 / 10 | time 0[s] | loss 0.71\n",
            "| epoch 67 |  iter 1 / 10 | time 0[s] | loss 0.71\n",
            "| epoch 68 |  iter 1 / 10 | time 0[s] | loss 0.71\n",
            "| epoch 69 |  iter 1 / 10 | time 0[s] | loss 0.70\n",
            "| epoch 70 |  iter 1 / 10 | time 0[s] | loss 0.68\n",
            "| epoch 71 |  iter 1 / 10 | time 0[s] | loss 0.73\n",
            "| epoch 72 |  iter 1 / 10 | time 0[s] | loss 0.66\n",
            "| epoch 73 |  iter 1 / 10 | time 0[s] | loss 0.69\n",
            "| epoch 74 |  iter 1 / 10 | time 0[s] | loss 0.66\n",
            "| epoch 75 |  iter 1 / 10 | time 0[s] | loss 0.70\n",
            "| epoch 76 |  iter 1 / 10 | time 0[s] | loss 0.65\n",
            "| epoch 77 |  iter 1 / 10 | time 0[s] | loss 0.67\n",
            "| epoch 78 |  iter 1 / 10 | time 0[s] | loss 0.70\n",
            "| epoch 79 |  iter 1 / 10 | time 0[s] | loss 0.63\n",
            "| epoch 80 |  iter 1 / 10 | time 0[s] | loss 0.66\n",
            "| epoch 81 |  iter 1 / 10 | time 0[s] | loss 0.65\n",
            "| epoch 82 |  iter 1 / 10 | time 0[s] | loss 0.66\n",
            "| epoch 83 |  iter 1 / 10 | time 0[s] | loss 0.64\n",
            "| epoch 84 |  iter 1 / 10 | time 0[s] | loss 0.62\n",
            "| epoch 85 |  iter 1 / 10 | time 0[s] | loss 0.62\n",
            "| epoch 86 |  iter 1 / 10 | time 0[s] | loss 0.63\n",
            "| epoch 87 |  iter 1 / 10 | time 0[s] | loss 0.59\n",
            "| epoch 88 |  iter 1 / 10 | time 0[s] | loss 0.58\n",
            "| epoch 89 |  iter 1 / 10 | time 0[s] | loss 0.61\n",
            "| epoch 90 |  iter 1 / 10 | time 0[s] | loss 0.59\n",
            "| epoch 91 |  iter 1 / 10 | time 0[s] | loss 0.58\n",
            "| epoch 92 |  iter 1 / 10 | time 0[s] | loss 0.57\n",
            "| epoch 93 |  iter 1 / 10 | time 0[s] | loss 0.55\n",
            "| epoch 94 |  iter 1 / 10 | time 0[s] | loss 0.54\n",
            "| epoch 95 |  iter 1 / 10 | time 0[s] | loss 0.53\n",
            "| epoch 96 |  iter 1 / 10 | time 0[s] | loss 0.54\n",
            "| epoch 97 |  iter 1 / 10 | time 0[s] | loss 0.51\n",
            "| epoch 98 |  iter 1 / 10 | time 0[s] | loss 0.51\n",
            "| epoch 99 |  iter 1 / 10 | time 0[s] | loss 0.50\n",
            "| epoch 100 |  iter 1 / 10 | time 0[s] | loss 0.47\n",
            "| epoch 101 |  iter 1 / 10 | time 0[s] | loss 0.49\n",
            "| epoch 102 |  iter 1 / 10 | time 0[s] | loss 0.46\n",
            "| epoch 103 |  iter 1 / 10 | time 0[s] | loss 0.44\n",
            "| epoch 104 |  iter 1 / 10 | time 0[s] | loss 0.47\n",
            "| epoch 105 |  iter 1 / 10 | time 0[s] | loss 0.44\n",
            "| epoch 106 |  iter 1 / 10 | time 0[s] | loss 0.43\n",
            "| epoch 107 |  iter 1 / 10 | time 0[s] | loss 0.43\n",
            "| epoch 108 |  iter 1 / 10 | time 0[s] | loss 0.39\n",
            "| epoch 109 |  iter 1 / 10 | time 0[s] | loss 0.40\n",
            "| epoch 110 |  iter 1 / 10 | time 0[s] | loss 0.41\n",
            "| epoch 111 |  iter 1 / 10 | time 0[s] | loss 0.38\n",
            "| epoch 112 |  iter 1 / 10 | time 0[s] | loss 0.38\n",
            "| epoch 113 |  iter 1 / 10 | time 0[s] | loss 0.38\n",
            "| epoch 114 |  iter 1 / 10 | time 0[s] | loss 0.37\n",
            "| epoch 115 |  iter 1 / 10 | time 0[s] | loss 0.36\n",
            "| epoch 116 |  iter 1 / 10 | time 0[s] | loss 0.34\n",
            "| epoch 117 |  iter 1 / 10 | time 0[s] | loss 0.35\n",
            "| epoch 118 |  iter 1 / 10 | time 0[s] | loss 0.33\n",
            "| epoch 119 |  iter 1 / 10 | time 0[s] | loss 0.35\n",
            "| epoch 120 |  iter 1 / 10 | time 0[s] | loss 0.33\n",
            "| epoch 121 |  iter 1 / 10 | time 0[s] | loss 0.33\n",
            "| epoch 122 |  iter 1 / 10 | time 0[s] | loss 0.32\n",
            "| epoch 123 |  iter 1 / 10 | time 0[s] | loss 0.31\n",
            "| epoch 124 |  iter 1 / 10 | time 0[s] | loss 0.31\n",
            "| epoch 125 |  iter 1 / 10 | time 0[s] | loss 0.31\n",
            "| epoch 126 |  iter 1 / 10 | time 0[s] | loss 0.30\n",
            "| epoch 127 |  iter 1 / 10 | time 0[s] | loss 0.30\n",
            "| epoch 128 |  iter 1 / 10 | time 0[s] | loss 0.27\n",
            "| epoch 129 |  iter 1 / 10 | time 0[s] | loss 0.30\n",
            "| epoch 130 |  iter 1 / 10 | time 0[s] | loss 0.28\n",
            "| epoch 131 |  iter 1 / 10 | time 0[s] | loss 0.26\n",
            "| epoch 132 |  iter 1 / 10 | time 0[s] | loss 0.27\n",
            "| epoch 133 |  iter 1 / 10 | time 0[s] | loss 0.27\n",
            "| epoch 134 |  iter 1 / 10 | time 0[s] | loss 0.28\n",
            "| epoch 135 |  iter 1 / 10 | time 0[s] | loss 0.26\n",
            "| epoch 136 |  iter 1 / 10 | time 0[s] | loss 0.28\n",
            "| epoch 137 |  iter 1 / 10 | time 0[s] | loss 0.25\n",
            "| epoch 138 |  iter 1 / 10 | time 0[s] | loss 0.26\n",
            "| epoch 139 |  iter 1 / 10 | time 0[s] | loss 0.26\n",
            "| epoch 140 |  iter 1 / 10 | time 0[s] | loss 0.26\n",
            "| epoch 141 |  iter 1 / 10 | time 0[s] | loss 0.23\n",
            "| epoch 142 |  iter 1 / 10 | time 0[s] | loss 0.23\n",
            "| epoch 143 |  iter 1 / 10 | time 0[s] | loss 0.26\n",
            "| epoch 144 |  iter 1 / 10 | time 0[s] | loss 0.23\n",
            "| epoch 145 |  iter 1 / 10 | time 0[s] | loss 0.24\n",
            "| epoch 146 |  iter 1 / 10 | time 0[s] | loss 0.24\n",
            "| epoch 147 |  iter 1 / 10 | time 0[s] | loss 0.25\n",
            "| epoch 148 |  iter 1 / 10 | time 0[s] | loss 0.21\n",
            "| epoch 149 |  iter 1 / 10 | time 0[s] | loss 0.23\n",
            "| epoch 150 |  iter 1 / 10 | time 0[s] | loss 0.22\n",
            "| epoch 151 |  iter 1 / 10 | time 0[s] | loss 0.22\n",
            "| epoch 152 |  iter 1 / 10 | time 0[s] | loss 0.23\n",
            "| epoch 153 |  iter 1 / 10 | time 0[s] | loss 0.23\n",
            "| epoch 154 |  iter 1 / 10 | time 0[s] | loss 0.20\n",
            "| epoch 155 |  iter 1 / 10 | time 0[s] | loss 0.22\n",
            "| epoch 156 |  iter 1 / 10 | time 0[s] | loss 0.21\n",
            "| epoch 157 |  iter 1 / 10 | time 0[s] | loss 0.21\n",
            "| epoch 158 |  iter 1 / 10 | time 0[s] | loss 0.20\n",
            "| epoch 159 |  iter 1 / 10 | time 0[s] | loss 0.21\n",
            "| epoch 160 |  iter 1 / 10 | time 0[s] | loss 0.20\n",
            "| epoch 161 |  iter 1 / 10 | time 0[s] | loss 0.19\n",
            "| epoch 162 |  iter 1 / 10 | time 0[s] | loss 0.22\n",
            "| epoch 163 |  iter 1 / 10 | time 0[s] | loss 0.19\n",
            "| epoch 164 |  iter 1 / 10 | time 0[s] | loss 0.21\n",
            "| epoch 165 |  iter 1 / 10 | time 0[s] | loss 0.20\n",
            "| epoch 166 |  iter 1 / 10 | time 0[s] | loss 0.20\n",
            "| epoch 167 |  iter 1 / 10 | time 0[s] | loss 0.20\n",
            "| epoch 168 |  iter 1 / 10 | time 0[s] | loss 0.19\n",
            "| epoch 169 |  iter 1 / 10 | time 0[s] | loss 0.18\n",
            "| epoch 170 |  iter 1 / 10 | time 0[s] | loss 0.19\n",
            "| epoch 171 |  iter 1 / 10 | time 0[s] | loss 0.19\n",
            "| epoch 172 |  iter 1 / 10 | time 0[s] | loss 0.20\n",
            "| epoch 173 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
            "| epoch 174 |  iter 1 / 10 | time 0[s] | loss 0.20\n",
            "| epoch 175 |  iter 1 / 10 | time 0[s] | loss 0.18\n",
            "| epoch 176 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
            "| epoch 177 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
            "| epoch 178 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
            "| epoch 179 |  iter 1 / 10 | time 0[s] | loss 0.18\n",
            "| epoch 180 |  iter 1 / 10 | time 0[s] | loss 0.19\n",
            "| epoch 181 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
            "| epoch 182 |  iter 1 / 10 | time 0[s] | loss 0.18\n",
            "| epoch 183 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
            "| epoch 184 |  iter 1 / 10 | time 0[s] | loss 0.18\n",
            "| epoch 185 |  iter 1 / 10 | time 0[s] | loss 0.18\n",
            "| epoch 186 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
            "| epoch 187 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
            "| epoch 188 |  iter 1 / 10 | time 0[s] | loss 0.18\n",
            "| epoch 189 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
            "| epoch 190 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
            "| epoch 191 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
            "| epoch 192 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
            "| epoch 193 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
            "| epoch 194 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
            "| epoch 195 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
            "| epoch 196 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
            "| epoch 197 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
            "| epoch 198 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
            "| epoch 199 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
            "| epoch 200 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
            "| epoch 201 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
            "| epoch 202 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
            "| epoch 203 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
            "| epoch 204 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
            "| epoch 205 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
            "| epoch 206 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
            "| epoch 207 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
            "| epoch 208 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
            "| epoch 209 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
            "| epoch 210 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
            "| epoch 211 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
            "| epoch 212 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
            "| epoch 213 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
            "| epoch 214 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
            "| epoch 215 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
            "| epoch 216 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
            "| epoch 217 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
            "| epoch 218 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
            "| epoch 219 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
            "| epoch 220 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
            "| epoch 221 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
            "| epoch 222 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 223 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
            "| epoch 224 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
            "| epoch 225 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
            "| epoch 226 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 227 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 228 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
            "| epoch 229 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
            "| epoch 230 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 231 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
            "| epoch 232 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 233 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 234 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
            "| epoch 235 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 236 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 237 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 238 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
            "| epoch 239 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
            "| epoch 240 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 241 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
            "| epoch 242 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 243 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 244 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
            "| epoch 245 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 246 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 247 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 248 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 249 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
            "| epoch 250 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 251 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 252 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 253 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 254 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 255 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 256 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 257 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 258 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 259 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 260 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 261 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 262 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 263 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 264 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 265 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
            "| epoch 266 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 267 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 268 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 269 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 270 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 271 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 272 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 273 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 274 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 275 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 276 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 277 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 278 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
            "| epoch 279 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 280 |  iter 1 / 10 | time 0[s] | loss 0.10\n",
            "| epoch 281 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 282 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 283 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 284 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 285 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 286 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 287 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 288 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 289 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 290 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 291 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 292 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 293 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 294 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 295 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 296 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
            "| epoch 297 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 298 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 299 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
            "| epoch 300 |  iter 1 / 10 | time 0[s] | loss 0.11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEJCAYAAACZjSCSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deVxc1f3/8dcHBoYdwg4BAmTfN0yiRo3GJcYlaq1ra63WpXVp7WL119a29ttqtdra1tatVhs17kvUqHGJUWM2YnayQxYg7An7OpzfH3NDSAIJJAyXYT7Px4NHZu69M/O5DJn3nHvuPUeMMSillPJdfnYXoJRSyl4aBEop5eM0CJRSysdpECillI/TIFBKKR+nQaCUUj7OY0EgIs+KSImIbOhk/bUisk5E1ovI1yIy3lO1KKWU6pwnWwTPAbOOsj4POMMYMxb4A/CUB2tRSinVCYenntgY84WIpB9l/dft7i4DUrryvLGxsSY9vdOnVUop1YFVq1aVGWPiOlrnsSDophuBD7qyYXp6OtnZ2R4uRyml+hcR2dXZOtuDQETOxB0E04+yzc3AzQBpaWm9VJlSSvkGW88aEpFxwDPAHGNMeWfbGWOeMsZkGWOy4uI6bNkopZQ6TrYFgYikAW8C3zXGbLWrDqWU8nUeOzQkIvOAGUCsiOQDvwUCAIwxTwD3ATHAv0QEoMUYk+WpepRSSnXMk2cNXX2M9T8AfuCp11dKKdU1emWxUkr5OA0CpZTycRoEQLOrlXkrdtPQ7LK7FKWU6nW2X0fQFzz9ZS4PfbgFY+CaqXqdglLKt2iLAHhv7V4A6ppabK5EKaV6n88Hwd7KenL2VgFQUt1oczVKKdX7fD4I1uVXtt3eW9lgYyVKKWUPnwuC8ppGHl24haqGZgDyymoBGJUUQbEGgVLKB/lcZ/FfP9nKC8t2U1jZwLcmpZBbWkNsmJNhCWGs2r2Pdfn7eWt1AVedlMbwxHC7y1VKKY8TY4zdNXRLVlaWOd5hqIsqGzj9oUUEB/pTWe9uEQQH+DN2YCSTBg3gicU78BNoNRAS6M+rt5zMmIGRPVm+UkrZQkRWdTaMj08dGvpgw16aXK3Mu2kaf7x0DKGB/tQ3u8iIDSUxwgm4Q+Dd290jYr+0Yred5SqlVK/wqSD4ekc5adEhjEqO4NqpgzhtqHtI64y4UBIjgwAYEBLA2JRIzhoRz0cbimhxtdpZslJKeZzPBIGr1bAst5xTBse0LZs5Mh6A9JhQEiLcQXDVFPcFZReMTaK8tonleRW9X6xSSvUin+ks3lBQSXVDCye3C4KLxidTWtPIjOFxBAX48/LN05iSHg3AjOHxOB1+fJxTzKlDYu0qWymlPM5nWgRlNY0kRQZxyuCDH+pBAf78aMYQggL8AZiWGYOfnwAQHOjPqUNi+WxzCd7Woa6UUt3hMy2CmSMTOGtEPNYkOF1y5oh4Pttcwo7SWobEh3mwOqWUso/PtAiAboUAwMwR8fgJPPjBZlyt2ipQSvVPPhUE3ZUcFcx9F47ik03FvLW6wO5ylFLKIzQIjuF7p6QTEujPxsLKY2+slFJeSIPgGESEzLhQdpTW2l2KUkp5hAZBF2TGhrGjpMbuMpRSyiM0CLpgcFwYhZX11DfpVJZKqf5Hg6ALMuNCMebgkNVKKdWfaBB0weA49zUEO0r18JBSqv/RIOiCzLhQwoMcegqpUqpf0iDoggNDUXy2uYQVOgidUqqf0SDoou+dMgiA5bnlNleilFI9S4Ogi0ICHTgdflQ3tthdilJK9SgNgm4IDwqg2pr0Ximl+guPBYGIPCsiJSKyoZP1IiJ/F5HtIrJORCZ5qpaeEhHsoKpBWwRKqf7Fky2C54BZR1l/PjDU+rkZ+LcHa+kR4UEBVNVri0Ap1b94LAiMMV8ARzvFZg7wP+O2DIgSkSRP1dMTIoIcVGuLQCnVz9jZRzAQ2NPufr617AgicrOIZItIdmlpaa8U15GIoACqtI9AKdXPeEVnsTHmKWNMljEmKy4uzrY6IoK1RaCU6n/sDIICILXd/RRrWZ+lfQRKqf7IziCYD1xnnT00Dag0xuy1sZ5jCnc6aGxppaml1e5SlFKqx3hs8noRmQfMAGJFJB/4LRAAYIx5AlgAzAa2A3XA9z1VS0+JCA4AoLqhmZgwp83VKKVUz/BYEBhjrj7GegPc5qnX94TwIPevq6qhRYNAKdVveEVncV8REXSwRaCUUv2FBkE3tLUI6vXMIaVU/6FB0A3t+wiUUqq/0CDohoN9BBoESqn+Q4OgG8KtPgI9NKSU6k80CLohIshBuNPBrgqdxF4p1X9oEHSDiDAyOYKNhVV2l6KUUj1Gg6CbRidHsHlvNa5WY3cpSinVIzQIuml0ciT1zS7mry3QTmOlVL+gQdBNo5IiALjrlbU8tTjX5mqUUurEaRB009CEMNKiQwDI31dnczVKKXXiNAi6KcDfj8W/mMG4lEj21emhIaWU99MgOA4iQny4k+KqBrtLUUqpE6ZBcJziwoMorW60uwyllDphGgTHKT7cSXltE80unaRGKeXdNAiOU0JEEABlNdoqUEp5Nw2C4xQf7p6YprhKg0Ap5d00CI5TfIQ7CEq0w1gp5eU0CI5TfLj70FCJdhgrpbycBsFxig0LxE9gd4VeVKaU8m4aBMfJ4e/HjOHxvJa9h7omnZ9AKeW9NAhOwG1nDmZfXTOvZefbXYpSSh03DYITMHlQNPHhTjYWVtpdilJKHTcNghOUFBXM3ko9c0gp5b00CE5QUkQQRRoESikvpkFwghIjDw2CK55YymOfbLOxIqWU6h6H3QV4u8TIIKobW6huaCbM6WBt/n4iggPsLksppbpMg+AEJUW6LywrrmqAiCAaW1opr9WLzJRS3sOjh4ZEZJaIbBGR7SJyTwfr00RkkYisFpF1IjLbk/V4QqI1+Nzeyoa2Yal1IDqllDfxWBCIiD/wOHA+MAq4WkRGHbbZr4FXjTETgauAf3mqHk9JigwGDg2C8pomO0tSSqlu8WSLYAqw3RiTa4xpAl4G5hy2jQEirNuRQKEH6/GIA4PPFVc2UGq1BOqaXHq1sVLKa3gyCAYCe9rdz7eWtfc74Dsikg8sAO7wYD0eERTgz6CYEJbsKKOs3QB02ipQSnkLu08fvRp4zhiTAswG5orIETWJyM0iki0i2aWlpb1e5LFcMyWNZbkVfLW9rG1Z6Qn0E9Q1teh8yEqpXuPJICgAUtvdT7GWtXcj8CqAMWYpEATEHv5ExpinjDFZxpisuLg4D5V7/K48KZWgAD8+2VTStuxEWgSPLtzKZf/6uidKU0qpY/JkEKwEhopIhogE4u4Mnn/YNruBmQAiMhJ3EPS9r/zHEBUSyMwRCQBEWtcQHO3Mocq6ZipqOw+KDYWVFOyvp6HZ1bOFKqVUBzwWBMaYFuB24CNgE+6zgzaKyP0icrG12c+Am0RkLTAPuN4YYzxVkyedPzYRgOqGZgDKOwiCPRV15BRWMf7+hVz+ROff+HeU1gLo4SGlVK/w6AVlxpgFuDuB2y+7r93tHOBUT9bQW84aEQ9AemwopVWNlNU08aMXV5EYEcx9F7nPmr31hVVsLKwCINf6sH9rdT6F+xu47cwhAFTWN7edhlpc1cigmNDe3hWllI/RK4t7SEiggzd+eDKJkcHc9Hw2X+8oY1tJDQDJUUFMTBvQFgLhTgfVjS3UNrbw3JKdbCmu5qbTMgl0+JFbWtP2nEXaIlBK9QINgh40eVA0AOePSeSRj7cCEOjw4//e34TT4T4K989rJtJq4M55q8krq2VTUTVNLa1sKKxkUtqAtpYCuK9NUEopT7P79NF+6eIJyYB7HKK3f3QqF4xNorGlFYCxAyNJiw4B4LPNJTRZy1fmVWCMIXtXBQ4/ISjAT1sESqleoS0CDxgUE8olE5IZnhjBqOQIfjlrBO+v30tEkIO06BDCnO4zhhas3wtAmNPByp0ViMC8FXu4eHwy6wsqtbNYKdUrNAg85G9XTWy7nRYTwriUSGJCAxERokMDCQn0Z3NRNWFOB+ePSWRhTjGlNU2MT43ir1dO4NpnlvHeur2MT8nlptMzbdwTpVR/p4eGeslz35/C3650h4OIUNfkvkZg5sh4pmREU1nfzNo9+5mWEY2/n+AnAsAfF2xiV3ltp88LkL+vjj0VdZ7dAaVUv6VB0EuiQwOJDDk4Yc1VJ6WSGBHE/10yhikZ0W3LJ6ZFAXDR+GQyY92njn6woajT531vXSHT/7yIMx5exPr8Sg9Vr5TqzzQIbPLAZWNZcs9ZhAcFkBYdQny4exTTiWkDALh6Shqf/XwG41IieeubArYVV/PIwi1c+eRSKuub257nk5xiokMDCQ8K4LFPt9qyL0op76ZBYBMRwd9P2m5PHxpLekwICdZENwdcd3I6W0uqOeevX/CPz7azYmcFv5u/EQBjDEt2lDN9SCw3nZbBJ5tK+MtHW6hvcvHckjyaXa1dqqWxxcV3nlnOql0VPbuTSimvoJ3FfcT9c8Z0OIfB5ZNTmJQWxerd+0mPDeWLraU89uk2/ER445t8AKYPiWXOxGRyS2v556Lt1DW5eHZJHslRwZw7OrHtuYwxbCysYnhiOAH+B78D7Cqv46vtZUzLjG67FkIp5Tu0RdBHhDkdxIcHdbguMy6Mb01OYfKgAfzozMFkxoa2hQDA9KGxOB3+/OTsYQC8s8Y9yOvS3HIaml08vmg7NY0tzF22iwv/8RWzH/uS11flt7UYdpe7O5rLdA4FpXyStgi8jNPhzyNXjOel5bv59QWjaGhxtR1OShkQTJjTQbk1sunSHeV8nFPMwx9tYUNBJZ9sKuak9AGU1zbx89fW8srK3Tz53Sz27HMHQflRRkRVSvVfGgReaGLagLZO5UgOnonk5yeMTApn5c59+PsJm4uq+XCj+4yjDzYUkRDh5JnrTiIi2ME7awq5+/V1/OadDSRYLZGORkwF94VvI5MiyIjVAfCU6o/00FA/MzLJPQX0nPHuYS7eX7eXQTEhJEYE8edvjSMyJAAR4ZKJA7lz5hDeX7eXl1bsAg6dTGdvZT2NLS4aW1zcMW81z3+9s9f3RSnVOzQI+pkDQXDppIGclO5uNVw2MYWl957FjOHxh2x78+mDiQ0LpKHZ3VdQXutuETQ0uzjn0S94/uud7C6vw9VqjjqRjlLKu2kQ9DOzxybx45lDmZoRwy2nDwZg+tAYxLpSub1Ahx8Xjktuu19e28QDCzaxMKeYmsYWthbXsMMaFntfnQaBUv2V9hH0M5HBAdx1jvvsobNHJfDl3WeSao122pHZY5N47uudBPgLzS7Dk1/kkhTp7jMo2FfPdmtOBW0RKNV/aYugnztaCABMyYjm2euz+L9LxrQt22vNg1Cwv75t2sx9GgRK9VsaBIqzRiSQFn3kGUF7K+vZVlINwL665iPWK6X6Bw0CBUBsWOARy5pdhg0FVfgJ1De7KKlu6PKwFUop79GlIBCRH4tIhLj9R0S+EZFzPV2c6j1x1qB3KQOCAQ65ZuDA2UZT/vgp1/93BbmlNdpnoFQ/0tUWwQ3GmCrgXGAA8F3gQY9VpXpdVEggL900lQ9/cjpXT0nlh2cMbls3Z8LBM4uWbC/nrEcWc+e81XaUqZTygK4GwYFzD2cDc40xG9stU/3EKYNjCXM6eOCycVw4PqlteVJk8BHbfrW9DGNMb5anlPKQrp4+ukpEFgIZwL0iEg7oweJ+LCTQwX0XjuKUITH4t7sG4afnDGNnWS1vri5gR2ktQ+LDbKxSKdUTuhoENwITgFxjTJ2IRAPf91xZqi+4YXoGAGXtxiC6c+ZQ8qwgWJ5XrkGgVD/Q1UNDJwNbjDH7ReQ7wK8BnRfRR0QFBxxy3z2BjpNlue6JbBZuLCJ7p05qo5S36moQ/BuoE5HxwM+AHcD/PFaV6lMc1iQ2Z490nz0kIkzNiGF5bjn5++q4ee4qrnhyqZ0lKqVOQFcPDbUYY4yIzAH+aYz5j4jc6MnCVN+y8ffnEeg4+L1hWmYM89cWcod19pDT4W9XaUqpE9TVIKgWkXtxnzZ6moj4AQHHeIzqR0Kdh/6pTM10T2m5evf+tmXGmA4Ht1NK9W1dPTR0JdCI+3qCIiAFePhYDxKRWSKyRUS2i8g9nWxzhYjkiMhGEXmpy5UrW2W2u+Ds5+cOo77ZRWV9M1uLq9lWXG1jZUqp7upSi8AYUyQiLwIniciFwApjzFH7CETEH3gcOAfIB1aKyHxjTE67bYYC9wKnGmP2iUh8x8+m+hoR4b07phMRFMD6Avd5A3srG7j79XUAvHvHdDvLU0p1Q1eHmLgCWAF8G7gCWC4ilx/jYVOA7caYXGNME/AyMOewbW4CHjfG7AMwxpR0p3hlrzEDI0mLCSEpyj1s9a7yOjYXVbGxsJLaxhabq1NKdVVXDw39CjjJGPM9Y8x1uD/kf3OMxwwE9rS7n28ta28YMExElojIMhGZ1cV6VB+SbF15vHhrCc0uQ6uBNXv2H+NRSqm+oqtB4HfYt/Xybjz2aBzAUGAGcDXwtIhEHb6RiNwsItkikl1aWtoDL6t6Uly4E38/4eOc4rZlq3bts7EipVR3dPXD/EMR+UhErheR64H3gQXHeEwBkNrufoq1rL18YL4xptkYkwdsxR0MhzDGPGWMyTLGZMXFxXWxZNVb/P2EhHAnZTVNhAT6MywhjPfWFeoIpUp5iS4FgTHmF8BTwDjr5yljzC+P8bCVwFARyRCRQOAqYP5h27yNuzWAiMTiPlSU2+XqVZ8xcdAAACYPGsAvzhvBzvI6bnvxG5urUkp1RZfnLDbGvAG80Y3tW0TkduAjwB941hizUUTuB7KNMfOtdeeKSA7gAn5hjCnv1h6oPuGfV0/kJzOHEhPmJDo0kNtmDOFvn26lpLqB+PAgu8tTSh2FHG0oYRGpBjraQABjjInwVGGdycrKMtnZ2b39sqqbNu2t4vzHvuSBy8Zy9ZQ0u8tRyueJyCpjTFZH6456aMgYE26MiejgJ9yOEFDeY0RiOCkDglm4scjuUpRSx6BzFiuPEBEuGJvEl9vKDhnGWinV92gQKI+5bFIKLa2Gd9YU2l2KUuooNAiUxwxPDGfswEjeWJVvdylKqaPQIFAedfnkFHL2VpFTWGV3KUqpTmgQKI+6eHwyAf7CvBW7Ka5qoKiywe6SlFKH6fJ1BEodjwGhgcwem8TcZbuYu2wXKQOC+fLuM3XeAqX6EG0RKI/787fG8eBlY0mKDCJ/Xz17KurtLkkp1Y4GgfK4oAB/rpqSxnPfnwLACp3oXqk+RYNA9Zqh8WFEBgewMk+DQKm+RINA9Ro/PyFr0AAWbSmhpFo7jZXqKzQIVK/60ZlDqGls4fpnV9La2vk4V0qp3qNBoHrV5EED+MOcMeTsreKr7WV2l6OUQoNA2eDC8UnEhAbyv6W77C5FKYUGgbKB0+HPpRMHsnhrCfVNLqobmu0uSSmfpkGgbDFp0ACaXYbbXvqGmY8spsXVandJSvksDQJli3EpkQB8trmEkupGNhdV21yRUr5Lg0DZYmBUMNGhgW33V+3aR2uroaHZZWNVSvkmDQJlCxFh7EB3q8Dp8CN71z7mrdzN9D9/RlOLHiZSqjfpoHPKNheMS6LVGCKCA1i1swKHn1BW00ReWS3DE8PtLk8pn6EtAmWbK7JSmXvjVCalDaCwsoGlO8oB2Fqs/QVK9SYNAmW78VbHcVGVe9iJbRoESvUqDQJlu1HJEfi1m55giwaBUr1Kg0DZLiTQwbAEd59AbJiTbcU1NleklG/RIFB9woHrCs4fk8jO8lqq9GpjpXqNBoHqE647OZ2fnD2Ub01OodXAO2sKyS3VloFSvUFPH1V9wpiBkYwZGIkxhsy4UH7z9gYA1tx3DlEhgcd4tFLqRGiLQPUpIsI1U9La7ueV1dpYjVK+waNBICKzRGSLiGwXkXuOst23RMSISJYn61He4cbpGbx268kA7Cqvs7kapfo/jwWBiPgDjwPnA6OAq0VkVAfbhQM/BpZ7qhblXQ4MPyHiDoJmVysbCyvtLkupfsuTLYIpwHZjTK4xpgl4GZjTwXZ/AP4M6CS2qk1QgD9JEUHsLK/l2meWc8Hfv2JXuR4mUsoTPBkEA4E97e7nW8vaiMgkINUY874H61BeKi0mhLdWF7AirwKANXv221yRUv2TbZ3FIuIHPAr8rAvb3iwi2SKSXVpa6vniVJ+QOiAEgJFJETgdfqzL18NDSnmCJ4OgAEhtdz/FWnZAODAG+FxEdgLTgPkddRgbY54yxmQZY7Li4uI8WLLqS5wB7j/PH84YzOjkCNZrECjlEZ4MgpXAUBHJEJFA4Cpg/oGVxphKY0ysMSbdGJMOLAMuNsZke7Am5UXuOGsov7lwFBeOTWJcShQbCiv5YP1eWluN3aUp1a94LAiMMS3A7cBHwCbgVWPMRhG5X0Qu9tTrqv4jISKIG6dn4OcnnJQeTV2Tix+++A33v5eDMRoGSvUU8bb/UFlZWSY7WxsNvsYYw87yOuYu3cWzS/J45roszh6VYHdZSnkNEVlljOnwWi29slh5BREhIzaUe2ePIDM2lAc+2ESLS6e0VKonaBAorxLg78c9549gR2ktTyzeYXc5SvULGgTK65w7OpELxiXx2Kfb2FOhQ1AodaI0CJRX+sW5w2l2GT7fUsJOHZhOqROiQaC80qCYEJIjg/jnou3M+MvnfLG1lA837KVZ+w2U6jYNAuWVRIRpmTEUVzUCcPfr67j1hW94eeWeYzxSKXU4DQLltaYNjgEg3OmgqMo9ZuErK3fbWZJSXkmDQHmtC8clcc/5I/jjZWMBmJgWxYaCKuY8voTtJTrNpVJdpVNVKq8VEujg1jMGY4whKjiAiWlR/OG9HN5ZU8h/vsrjASsglFJHpy0C5fVEhNOHxREeFMBDl4/n4vHJvLOmgOqGZrtLU8oraBCofufaaYOoa3Lx0nLtL1CqK/TQkOp3JqRGcfqwOP71+Q6iQwNZmFPMXWcPY1RyhN2lKdUnaYtA9Uu/nDWchmYXv3h9HR/nFPP3T7fZXZJSfZYGgeqXRidHsuSes3j/zun8YHoGC3OKWLS5hPomF+f+dTHvrSu0u0Sl+gwNAtVvxYY5GZ0cyfdOSSc4wJ/vP7eSX729nq3FNbyanW93eUr1GRoEqt9LjQ7h81+cSWp0MO+scbcElu0op6axxebKlOobNAiUT4gLdzItIwaXNc1lk6uVr7aVHbHdb97eoIeNlM/RIFA+Y0pGNADTh8QSHuTg003FgHv2s7lLd7JyZwVzl+3i7dUFNlapVO/T00eVz5ia4R6baFJaFANCA1m0pYQWVysPL9zCk4tziQwOAGBHqQ5rrXyLBoHyGWkxIfzr2klMy4zhi62lvLu2kPP+9gU7SmuJDg2korYJgF3ltTS2uHA6/G2uWKneoUGgfMrssUkAnDEsjkB/P/bXNfO3KycQH+7kmmeWA9Bq4IbnVhLk8OeUIbFckZVCeFCAnWUr5VEaBMonDQgN5P07pxMfHkRkSADNrlYSIpwMSwjny21lLNleTmxYIJ9uLmFjYSWPXjGBl1fsJsTp4OLxyXaXr1SP0s5i5bOGJoQTGeL+ph/g78fCn5zBP6+Z1Lb+05/OYPbYRFbkVWCM4aGPtvDk4h1t6xtbXDS2uHq9bqV6mrYIlLIcCIWRSRGMSY4gMiSAsQOjWLC+iFW79lFR20RNYwuuVoO/n3D2o4sJdwaw4Men2Vy5UidGg0Cpwyy4c3rb7bEDIwH475KdADS1tLJyZwWl1Y3sqagH6jHGICI2VKpUz9AgUOow7T/UR1sjlr6/fi9+4u5IvuqpZYdsX1TVQGyYkwB/P8prGimvbWJYQniv1qzUidA+AqWOYkBoIEmRQQBcPjmlbfm3J6fwoxmDAbjm6eVc+q8ltLhaueG5lVz0j6/YWlxtS71KHQ8NAqWO4enrsph30zQevGxc27LfzxnN9aekA5BXVsuGgipuf2k1a/MraTWGu19fZ1O1SnWfHhpS6hjGWP0EAI9eMR4/EUICHQQH+BMe5KC6oYXwIAcfbixifGoU541O4KEPt7CtuJq6JhfjU6OOeE5jDI0trQQF6EVryn4eDQIRmQU8BvgDzxhjHjxs/U+BHwAtQClwgzFmlydrUupEXDbp4OEhEWFIfBhbi6p5/47TKKpqYFJaFDl7q3iILVzx5FL21TUzPjWKWaMTuf6UdHZV1DIiMYI3ving/nc38vW9MwkJ8MfPTzublX08FgQi4g88DpwD5AMrRWS+MSan3WargSxjTJ2I/BB4CLjSUzUp1dNuPWMw++uaSIsJIS0mBHBPihMe5GBfXTNT0qOpamjmzx9u5u3VBWwtqea9O6bz1bZSqhpa+NvHW3k1ew8L7zqDRKsvQqne5sk+ginAdmNMrjGmCXgZmNN+A2PMImNMnXV3GZCCUl7kvNGJXHlS2iHL/P2EqRnRiMAjV4zn7dtOJT7cyZbiaoyB+9/NYV1+JQBzl+2iqqGFp7/M5fMtJVTWN9uxG8rHeTIIBgJ72t3Pt5Z15kbgAw/Wo1Sv+cnZw/jL5eNJjQ4hKMCf3140mjOHx/H/Zo9geV4FuWXuEU4bW1oB+M9XeVz/35VMf/AzdpUfHP10WW45Mx/5nPx9dVQ3aEgoz+gTZw2JyHeALODhTtbfLCLZIpJdWlrau8UpdRzGDIzkW+1ON71gXBL//f4Urp06iDCn+4is0+H+73f+mEROGxrL366cQKOrlSesYSw27a3iqqeWsaO0lue/3snY3y3k/XV7e39nVL/nySAoAFLb3U+xlh1CRM4GfgVcbIxp7OiJjDFPGWOyjDFZcXFxHilWqd4Q6nTwrUkD8ROYNSYRgO+dks7cG6dyycSBXJGVwuur8pm/tpAL//FVW2i8+Y37v84jC7cc8nx7K+s56Y+fsGhLSe/uiOpXPBkEK4GhIpIhIoHAVcD89huIyETgSdwhoH/JyifcPWsEr916MldmpTIhNYoJ7U4vveX0wbQauOuVNUSHBvLl3WcyKCaEcmuuhNyyWjYXVbVt/9ySnZRWN/LS8t0dvlZTSytXPrmUd9borGuqcx4LAmNMC3A78BGwCXjVGLNRROFWGRUAABHrSURBVO4XkYutzR4GwoDXRGSNiMzv5OmU6jdCnQ4mD4rmlCGxvH3bqYdcS5AaHcKcCcm4Wg3fPzWdAaGBZMaGApAZG0p4kIM/vr8JYwyF++t5acVuHH7C4i2lzFuxm8q6Zv768VbOeHgRLyzbxbtrC1meV8E7aw7Ow2yMwRhzRF0l1Q3sswJH+RaPXkdgjFkALDhs2X3tbp/tyddXyhv99Jxh+IvwnWmDABgcF8aiLaVMzYxhWEIYv383h1H3fYSfgJ8If7x0DL98Yz33vrmezzaX8HFOMbFhTu57ZwPRoYEArNq1j9ZWw7aSGm6Zm81F45P52bnD217TGMN3nllOYmQw/7thii37reyjVxYr1cekDAjh4W+Pb7ufGRcGwPCEML57cjqhTgdbi6qprG/mhukZjEyKICokkHveWMfHOcUAvHrLNP76yTby99Vx6pBY3llTyPNLd/Lowq1UN7pPV/3+qRlEhwbyxOId1De52FpcQ15ZLbWNLYQ6D340bC+pob7JxdiUSMprGqluaCHdaqWo/kGDQKk+bnxqJP5+QlZ6NP5+whVZqUdsc97oRLaX1PDwR1sYnhBOZlwY/7h6IgA7y2p5Z00hv383h2EJYfxj9kiu/+9Krnl6GRPTopi34uBZ3s0uw9Id5Zw9KoHaxhZcxnDz/7JpaHax4MenMfn/PsFPYMefZp/Q0NvGGPLKattCTtlLg0CpPm50ciRr7jvnmPMmnzEsjoc/2sLpw2IPWT4oJoSzR8aTGBnEL2eNIDwogPsuHMX8tYXMW7GHzLhQSqsayYwPY1txNYu3llLb1MJv3t6Aq9VQ2+Sehe2G51YC7qG4l+dVsDy3gvAgBzdMzziiFmMMVQ0tRAZ3XPNHG4u59YVVfHzX6QzVIbttJx11GvVlWVlZJjs72+4ylOpzjDG8sHw3545KICGia8NVbCioJD7cSUVdE6GBDh74YBPLcitwtRpSo4MprmrEGCircZ/ZPXNEPJ9uLiHQ4UeTdTHcb6xQaWx2MTg+jL9eMYHnvs7j759u5+Ofns685bvZUVbLI98e39Yxfu+b65m3YjePXTWBOROOdp2p6ikissoYk9XROm0RKNVPiAjftTqYu+rAyKrxVnBcPSWNBeuLAHjygsmMTIygpbWVi/+5hIL99dw7eyQ5e6vYW9nALWdk8syXefzhvRzSokPIiA3l/XV7GZ8SyfNf76KmsYVrnl5OnnUV9XmjE0mLDuG38zdSVFkPwI6SGspqGnnwg81cPSWVyYOiu1z7noo6EiKCCHT0ietivZr+BpVSbU4dHEtGbChD48OYmhFNZEgAMWFOvnvyIK46KZUh8WHMGB5PYkQQP5k5jJkj4nH4Cc98L4vnb5jCjOFxPPjBZgr21+Mn7rkazh2VQHJkEG9+k8+/P9/O2j37Ka5ytzB2lNXyyso9vL4qn8ufWMqKvAqeW5JHSVUD4O6obna1ttXnajVsKKgkf18dMx9dzL8/d1+Fnb2zgorDTn2tbmimscXVS78576aHhpRSh9hTUYeI++yljjQ0u2hodhEVEkhJdQN7KuravsmXVjfy10+2UlTZQFyYk1ey9/DCjVNZmlvW9qGdHhNKXnktGTGhBDr8cLUanAF+5JbWEup0UFrdSGyYk8snp/DE4h2kx4Tw7PUn4Qzw54onllKwv564cCel1Y1kxoXy+DWTuODvX3LVlDQuHJtEfISTQTGhnPmXz5k+JJZ7Z4+krKaRQdEhOPw7/u5bUt3QNh91XmktPz13OG+symd5XjkPXT6+w8d4m6MdGtIgUEp5xJ6KOt5fv5ebT8ukurGF//fWer7cWsqCH5+Gw8+PZ5fk8dQXuQD84ZIxrNpZwdtrChkSH0ZrqyG3rJYJqVHsKKlhamY0pTVN5JbUcOqQWD7cWNTWTzEkPoztJTXEhAayv74ZfxFmjUlk/tpCwpwOokMD2V1RR9agAbzwg6m8vGI3zgB/rp7iHjV27Z79zHl8CXHhTpIjg9hQWMU3vz6Hm+dmszyvggcuG8uGgkr+eOnYY+5zdUMzH6wv4vLJKX1ujgntI1BK9brU6BBuPcM9r3NkcACPXzMJY0zbaacZ1rUITocfF49PJj0mhLfXFHLHWUM4a0Q8r2Xnc8nEgfznq1weX7QDEfj3tZPdZ0W9AheNT+bHL68mr6yWU4fEsGR7OQATBkUxf20hTocfNY0t1DS2cO3UNF5cvpvrnl3Byp0VAOTvqyM+PIi/f7oNcLdmKmqbcLUaPt1c3DZU+G/f2UiTq5W06BA+3VzC3BunUFLVSGr0wRbTF1tLeW1VPsPiw3jk461EhgRw8uAYXly2m8snp/D0l7ncOXMoYU4HeyrqDnlsX6AtAqWULQr31/OH93K49/yRbZP6bCmqZlhC2CHXKJTXNHLT/7L57smDuHTioVOW5BRWERMWSKsxnPzAZ0zJiGbujVP48wdbmJoZza/e2kBmbCiv3DKN/y3dxe/f3Uh8eBDBgf5tndhhTgf3zxnNT19d2/a8B1oZ7QX6+9HkauXkzBiW5pYzJSOa2WMSKdhfz9Nf5gEQEeSgqqGFqRnRnD8mkd+9m8OE1CjW7NnPny4dS3RoILe+sIpnrsti+tDYtkNsh2todvHgB5u5YFwSJ6V3vQP9aPTQkFKq33vqix2clB7NxLQBbct2ldcSHhTQNtTG2j37CXU6iAtzUt/somB/HUEB/gxPCGfc7xdS1+TivNEJfLTRfYV2anQweyrqcfgJLa0HPytHJIZT3dBCwf56nA4/RidH8M3u/QCEOx1UN7aQGRvaNu8EwKS0KEprGtlTUc+opAj2VNRR3djCd6alERcWxNLcMjYWVDFzZDw1jS18sqmE8CAHwxPCaWhxcd3J6R1eTNhVemhIKdXv3Xz64COWDYo5dCiM8e1Geo0k4JDpQU9Kjya3rIaHvz2ejzYuBOCeWSP5ansZu8pr+XpHOcmRQRRWNnDv7JGcOjiGoqoGkiKD8fcTfvjCKj7YUMQvZg3nwQ82k1tWS4C/0OwyxIY524Ji8qABrNq1j7hwJ7PHJvHCst2IwKikCM4aGc+C9UU0t7Zy02kZfJxTTHOrwRi4+/V1VNU384PTMnv8d6dBoJRSwAOXjaW2sYWIoABW/fps9tU1MSQ+nAvGJfHS8t3UNbm465xhvLO6gNOGxOLnJ4ecWTVnQjKLt5Yya3Qi24prmLtsFz89Zzgf5xRx7+yR/GnBJm4/cwjJUcFc/M+v+N1Fo7lgXBI3nZ5BbJiz7RDRny5tAdyj1P7qglEANLtauffN9YxIjPDIvuuhIaWU6iGNLS6cDn+Kqxp4cnEud88afsgw4wfUN7kIDjxyuSfpoSGllOoFTof7wz0hIoj7LhrV6Xa9HQLHolcWK6WUj9MgUEopH6dBoJRSPk6DQCmlfJwGgVJK+TgNAqWU8nEaBEop5eM0CJRSysd53ZXFIlIK7DrOh8cCZT1Yjp10X/om3Ze+SfcFBhlj4jpa4XVBcCJEJLuzS6y9je5L36T70jfpvhydHhpSSikfp0GglFI+zteC4Cm7C+hBui99k+5L36T7chQ+1UeglFLqSL7WIlBKKXUYnwkCEZklIltEZLuI3GN3Pd0lIjtFZL2IrBGRbGtZtIh8LCLbrH8HHOt57CAiz4pIiYhsaLesw9rF7e/W+7RORCbZV/mROtmX34lIgfXerBGR2e3W3WvtyxYROc+eqo8kIqkiskhEckRko4j82Frude/LUfbFG9+XIBFZISJrrX35vbU8Q0SWWzW/IiKB1nKndX+7tT79uF7YGNPvfwB/YAeQCQQCa4FRdtfVzX3YCcQetuwh4B7r9j3An+2us5PaTwcmARuOVTswG/gAEGAasNzu+ruwL78Dft7BtqOsvzUnkGH9DfrbvQ9WbUnAJOt2OLDVqtfr3pej7Is3vi8ChFm3A4Dl1u/7VeAqa/kTwA+t2z8CnrBuXwW8cjyv6ystginAdmNMrjGmCXgZmGNzTT1hDvC8dft54BIba+mUMeYLoOKwxZ3VPgf4n3FbBkSJSFLvVHpsnexLZ+YALxtjGo0xecB23H+LtjPG7DXGfGPdrgY2AQPxwvflKPvSmb78vhhjTI11N8D6McBZwOvW8sPflwPv1+vATBGR7r6urwTBQGBPu/v5HP0PpS8ywEIRWSUiN1vLEowxe63bRUCCPaUdl85q99b36nbrkMmz7Q7RecW+WIcTJuL+9unV78th+wJe+L6IiL+IrAFKgI9xt1j2G2NarE3a19u2L9b6SiCmu6/pK0HQH0w3xkwCzgduE5HT26807rahV54C5s21W/4NDAYmAHuBR+wtp+tEJAx4A/iJMaaq/Tpve1862BevfF+MMS5jzAQgBXdLZYSnX9NXgqAASG13P8Va5jWMMQXWvyXAW7j/QIoPNM+tf0vsq7DbOqvd694rY0yx9Z+3FXiag4cZ+vS+iEgA7g/OF40xb1qLvfJ96WhfvPV9OcAYsx9YBJyM+1Ccw1rVvt62fbHWRwLl3X0tXwmClcBQq+c9EHenynyba+oyEQkVkfADt4FzgQ249+F71mbfA96xp8Lj0lnt84HrrLNUpgGV7Q5V9EmHHSu/FPd7A+59uco6syMDGAqs6O36OmIdR/4PsMkY82i7VV73vnS2L176vsSJSJR1Oxg4B3efxyLgcmuzw9+XA+/X5cBnVkuue+zuJe+tH9xnPWzFfbztV3bX083aM3Gf5bAW2HigftzHAj8FtgGfANF219pJ/fNwN82bcR/fvLGz2nGfNfG49T6tB7Lsrr8L+zLXqnWd9R8zqd32v7L2ZQtwvt31t6trOu7DPuuANdbPbG98X46yL974vowDVls1bwDus5Zn4g6r7cBrgNNaHmTd326tzzye19Uri5VSysf5yqEhpZRSndAgUEopH6dBoJRSPk6DQCmlfJwGgVJK+TgNAqWU8nEaBMoricjX1r/pInJNDz/3/+votTxFRC4RkfuOsc23rWGJW0Uk67B1RwypLCKBIvJFu6tRleqUBoHySsaYU6yb6UC3gqALH46HBEG71/KUu4F/HWObDcBlwBftF4rIKNxXyo8GZgH/EhF/4x5l91Pgyp4vV/U3GgTKK4nIgaF6HwROsyYeucsaufFhEVlpjTp5i7X9DBH5UkTmAznWsret0Vw3HhjRVUQeBIKt53ux/WtZwys8LCIbxD1J0JXtnvtzEXldRDaLyIsHhgIWkQfFPWHKOhH5Swf7MQxoNMaUWfffEZHrrNu3HKjBGLPJGLOlg1/F0YZUfhu49oR+0conaLNRebt7cE8+ciGA9YFeaYw5SUScwBIRWWhtOwkYY31gAtxgjKmwxnRZKSJvGGPuEZHbjXv0x8Ndhnsky/FArPWYA9/QJ+L+Vl4ILAFOFZFNuMe4GWGMMQfGkDnMqcA37e7fbNWcB/wM96QkRzMQWNbufvshijcAJx3j8Uppi0D1O+fiHhxtDe4x6WNwDyoGsKJdCADcKSJrcX+QprbbrjPTgXnGPaJlMbCYgx+0K4wx+cY90uUa3IesKoEG4D8ichlQ18FzJgGlB+5Yz3sf7kHGfmaM6eokOEcwxriApgMDFirVGQ0C1d8IcIcxZoL1k2GMOdAiqG3bSGQGcDZwsjFmPO6BvoJO4HUb2912AQ7jnihkCu6Zoy4EPuzgcfUdvO5Y3EMJJ3fhdY81pLITdxgp1SkNAuXtqnHPU3vAR8APrfHpEZFh1tDdh4sE9hlj6kRkBIcegmk+8PjDfAlcafVDxOGev7jT4YutiVIijTELgLtwH1I63CZgSLvHTME9+dBE4OfWMMlH0+mQyiISA5QZY5qP8RzKx2kQKG+3DnCJyFoRuQt4Bndn8DcisgF4ko77wj4EHNZx/Ac59Dj7U8C6Ax217bxlvd5a4DPgbmNM0VFqCwfeE5F1wFfATzvY5gtgotUR7cQ9gcoNxphC3H0Ez1rrLhWRfNyTlLwvIh8BGGM24p7YPMfap9usQ0IAZwLvH6U+pQB0GGql7CYijwHvGmM+6eHnfRO4xxiztSefV/U/2iJQyn5/AkJ68gnFPRPf2xoCqiu0RaCUUj5OWwRKKeXjNAiUUsrHaRAopZSP0yBQSikfp0GglFI+7v8DVxJ8s/9ftSMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc-qUEBkvjpw",
        "colab_type": "text"
      },
      "source": [
        "1.5 計算の高速化\n",
        "* ニューラルネットワークはいかに高速に計算するかということが重要なテーマとなる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_khAi2QNwHXf",
        "colab_type": "text"
      },
      "source": [
        "1.5.1 ビット精度\n",
        "* Numpyでは標準で64ビットの浮動小数点数が使われる。\n",
        "* しかし、ニューラルネットワークの推論および学習は32ビットの浮動小数点数で問題なく行えることが知られている。\n",
        "* Numpyで32ビット浮動小数点数を使うには、次のようにデータの型をnp.float32や'f'と指定する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GT3cScxwNjD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fd64afd8-b61a-480f-d090-c7164c21d376"
      },
      "source": [
        "import numpy as np\n",
        "a = np.random.randn(3)\n",
        "a.dtype"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwVHcOeHwgeb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b40adb66-eebd-43f0-8fee-6a1491a00e1f"
      },
      "source": [
        "b = np.random.randn(3).astype(np.float32)\n",
        "b.dtype"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djDKgqKgwwxz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "af989ea8-46ca-4ad9-b08d-da516c9dacf2"
      },
      "source": [
        "c = np.random.randn(3).astype('f')\n",
        "c.dtype"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeRWwscxymhb",
        "colab_type": "text"
      },
      "source": [
        "1.5.2 GPU(Cupy)\n",
        "* ここから先はGoogle ColabのランタイムをGPUに切り替える。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAlL4DqZ06kj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "35fdea66-1bd3-4a8a-fa39-7b0f0c57fb72"
      },
      "source": [
        "import cupy as cp\n",
        "x = cp.arange(6).reshape(2, 3).astype('f')\n",
        "x"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 2.],\n",
              "       [3., 4., 5.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeEdBtO_1Zkw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5542669b-a025-460a-89ba-663183ef9c82"
      },
      "source": [
        "x.sum(axis=1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3., 12.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpiqOLat1pKU",
        "colab_type": "text"
      },
      "source": [
        "1.6 まとめ\n",
        "* ニューラルネットワークの基本的な復習です。"
      ]
    }
  ]
}